# -*- coding: utf-8 -*-
"""NLP_Project_algos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10nHEuEMMUHBsbiZDJxBMJtSnYv3Zr3W0
"""

import numpy as np
import pandas as pd
import os
import warnings
import math
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('punkt')
import gensim
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM
warnings.filterwarnings('ignore')
dataset = pd.ExcelFile("alltweet.xlsx")
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

tweet = pd.read_excel(dataset, sheet_name = "Sheet1")

# Tokenize all the tweets
tokens = []
for i in range(len(tweet)):
    text = tweet.iloc[i,0].lower()
    text = text.replace('"', "")
    text = re.sub("RT @.*: ", "", text)
    text = re.sub("https://.* ", "", text)
    text = re.sub("https://.*(\s*)", "", text)
    text = re.sub("&amp; *", "", text)
    text = re.sub("&gt; *", "", text)
    text = re.sub(r'[^\x00-\x7f]',r' ',text)
    token = nltk.word_tokenize(text)
    tokens.append(token)

# generate the language model using gensim
gen_model = gensim.models.Word2Vec(tokens, size=200, window=10, min_count=5, workers=8)

# transform our data to ready-to-use features
def construct_feature(df):
    result = []
    for j in range(4, 13):
        oneset = []
        for i in range(len(df)):
            oneline = []
            text = df.iloc[i,0]
            newvec = np.zeros(200)
            text_token = [temp.lower() for temp in nltk.word_tokenize(text)]
            for k in text_token:
                if k in gen_model:
                    newvec = newvec + gen_model[k]
            oneline.append(newvec)
            oneline.append(df.iloc[i,1])
            oneline.append(df.iloc[i,2])
            oneline.append(df.iloc[i,3])
            oneline.append(df.iloc[i,j])
            oneset.append(oneline)
        result.append(oneset)
    return np.array(result)

trainingset = construct_feature(train)
testingset = construct_feature(test)

# separating features and classes
def separate_fc(arr):
    target = []
    feature = []
    for lines in arr:
        temp1 = [lines[-1]]
        temp2 = lines[0].tolist()
        for p in range(1, len(lines)-1):
            temp2.append(lines[p])
        target.append(temp1)
        feature.append(temp2)
    return np.array(feature), np.array(target)

# Calculate the Cross-Validation accuracy for different classifiers
def cv_accuracy(clf, k, X, Y):
    kf = KFold(n_splits = k)
    accuracy = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        Y_train, Y_test = Y[train_index], Y[test_index]
        clf.fit(X_train, Y_train)
        predicted = clf.predict(X_test)
        accuracy.append(accuracy_score(Y_test, predicted))
    return np.mean(accuracy)

# find the weight for Naive Bayes classifier
def find_nb(X, Y):
    naivebayes = GaussianNB()
    accu = cv_accuracy(naivebayes, 4, X, Y)
    return accu

# Find the weight and the corresponding optimal SVM classifier
def find_optimal_svm(X, Y):
    choice_of_lambda = [0.1, 1, 10]
    choice_of_tolerance = [1e-04, 1e-05, 1e-06]
    accu = 0
    optimal_l = 0.0
    optimal_t = 0.0
    for l in choice_of_lambda:
        print("Check Point 1")
        for t in choice_of_tolerance:
          svm = SVC(tol = t, C = l, max_iter = 2000, kernel='linear',probability=True)
          new_accu = cv_accuracy(svm, 2, X, Y)
          if new_accu > accu:
            accu = new_accu
            optimal_l = l
            optimal_t = t
    result = SVC(tol = optimal_t, C = optimal_l, max_iter = 2000, kernel='linear',probability=True)
    return accu, [optimal_t, optimal_l], result

# Find the weight and the corresponding optimal Random Forest classifier
def find_optimal_rf(X, Y):
    choice_of_tree = [70, 100, 130]
    choice_of_minsplit = [3, 6, 9]
    choice_of_method = ['gini', 'entropy']
    accu = 0
    optimal_t = 0
    optimal_s = 0
    optimal_m = ""
    for t in choice_of_tree:
        print("Check Point 2")
        for s in choice_of_minsplit:
            for m in choice_of_method:
                rf = RandomForestClassifier(n_estimators = t, criterion = m, min_samples_split = s, random_state=0)
                new_accu = cv_accuracy(rf, 2, X, Y)
                if new_accu > accu:
                    accu = new_accu
                    optimal_t = t
                    optimal_s = s
                    optimal_m = m
    result = RandomForestClassifier(n_estimators = optimal_t, criterion = m, min_samples_split = optimal_s, random_state = 0)
    return accu, [optimal_t, optimal_s, optimal_m], result

#print("Optimal parameters for SVM:", svm_optimal)

#print("Optimal parameters for RF:", rf_optimal)

# Test the performance of Adaboost classifier with Naive Bayes as base estimator 
def test_adaboost_nb(clf, X_train, Y_train, X_test, Y_test):
    boost = AdaBoostClassifier(base_estimator = clf, n_estimators = 100)
    boost.fit(X_train, Y_train)
    predicted = boost.predict(X_test)
    print(metrics.classification_report(Y_test, predicted))

# Test the performance of Voting Classifier using Naive Bayes, SVM, Random Forest as base estimators
def test_voting(clf1, clf2, clf3, weight, X_train, Y_train, X_test, Y_test):
    vot = VotingClassifier(estimators=[('nb', clf1), ('svm', clf2), ('rf', clf3)],voting='soft', weights=weight)
    vot.fit(X_train, Y_train)
    predicted = vot.predict(X_test)
    print(metrics.classification_report(Y_test, predicted))

# Find the optimal classifiers and test the results on testing set
for i in range(9):
  training_f, training_c = separate_fc(trainingset[i])
  testing_f, testing_c = separate_fc(testingset[i])
  nb_weight= find_nb(training_f, training_c)
  nb_clf = GaussianNB()
  print("NB Done!")
  svm_weight, svm_optimal, svm_clf = find_optimal_svm(training_f, training_c)
  print("SVM Done!")
  rf_weight, rf_optimal, rf_clf = find_optimal_rf(training_f, training_c)
  print("RF Done!")
  print("Naive Bayes with Adaboost: class: ", i)
  test_adaboost_nb(nb_clf, training_f, training_c, testing_f, testing_c)
  print("Voting: class: ", i)
  test_voting(nb_clf, svm_clf, rf_clf, [nb_weight, svm_weight, rf_weight], training_f, training_c, testing_f, testing_c)

# Training for the optimal batch size and testing the classifier on testing set
def RNN_LSTM(train, test):
    # building word index
    alltxt = train.iloc[:,0].values.tolist() + test.iloc[:,0].values.tolist()
    tokenizer = Tokenizer(num_words=2000, lower=True)
    tokenizer.fit_on_texts(np.array(alltxt))
    word_index = tokenizer.word_index
    XX_train = tokenizer.texts_to_sequences(train.iloc[:,0].values)
    XX_train = pad_sequences(XX_train, maxlen=50)
    XX_test = tokenizer.texts_to_sequences(test.iloc[:,0].values)
    XX_test = pad_sequences(XX_test, maxlen=50)
    
    #generating dummy label
    dummy_label_train = []
    dummy_label_test = []
    for i in range(4, 13):
        temp = pd.Series(train.iloc[:, i].values.tolist()+test.iloc[:,i].values.tolist())
        temp2 = pd.get_dummies(temp).values
        dummy_label_train.append(temp2[:len(train)])
        dummy_label_test.append(temp2[len(train):])
    
    kf = KFold(n_splits = 3)
    choice_of_batch = [20, 24, 28, 32, 36]
    accuracy_all = []
    optimal_class = 0
    optimal_batch_all = []
    
    for i in range(8, 6, -1):
        accuracy = 0.0
        optimal_batch = 0
        print("Working on ",i)
        for batch_size in choice_of_batch:
            accu = []
            for train_index, test_index in kf.split(XX_train):
                X_train, X_test = XX_train[train_index], XX_train[test_index]
                Y_train, Y_test = dummy_label_train[i][train_index], dummy_label_train[i][test_index]
                model = Sequential()
                model.add(Embedding(2000, 100))
                model.add(LSTM(25, dropout=0.2, recurrent_dropout=0.2))
                model.add(Dense(3, activation='softmax'))
                model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
                model.fit(X_train, Y_train, epochs=20, batch_size=batch_size)
                accu.append(model.evaluate(X_test,Y_test)[1])
            if accuracy < np.mean(accu):
                accuracy = np.mean(accu)
                optimal_batch = batch_size
        accuracy_all.append(accuracy)
        print("Optimal batch for ", i, " is ", optimal_batch)
        optimal_batch_all.append(optimal_batch)
    
    test_accuracy = []
    for i in range(9):
        model = Sequential()
        model.add(Embedding(2000, 100))
        model.add(LSTM(25, dropout=0.2, recurrent_dropout=0.2))
        model.add(Dense(3, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
        model.fit(XX_train, dummy_label_train[i], epochs=20, batch_size = optimal_batch_all[i])
        test_accuracy.append(model.evaluate(XX_test, dummy_label_test[i])[1])
        
    return test_accuracy

LSTM_result = RNN_LSTM(train, test)

print(result)

